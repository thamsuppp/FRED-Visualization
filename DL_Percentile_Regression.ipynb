{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL - Percentile Regression.ipynb",
      "provenance": [],
      "mount_file_id": "1BZkduJVxvGx3J39WR2v-Wuy7pokLMF75",
      "authorship_tag": "ABX9TyPyFYaDJASui1aJRzC4/VpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thamsuppp/FRED-Visualization/blob/master/DL_Percentile_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R0hQULBhOOv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import sklearn\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3yE3GJY7SLq"
      },
      "source": [
        "Last Updated 2 Dec\n",
        "\n",
        "**Housekeeping**\n",
        "\n",
        "1. Download tensorflow_gpu (to enable much quicker training)\n",
        "2. Download eli5\n",
        "3. Download scikit-learn==0.21.3 (to enable text highlighting visualization of the eli5 explanations) https://github.com/TeamHG-Memex/eli5/issues/361\n",
        "\n",
        "**Workflow**\n",
        "\n",
        "1. Preprocessing raw text data\n",
        "2. Loading existing word embeddings to create embedding matrix\n",
        "3. Train RNN model (GRU) to regress on percentiles\n",
        "4. Evaluating Model (MSE)\n",
        "5. Explainable Model Insights (contribution of each word to prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wPjyw5RiRue",
        "outputId": "0351b9a6-f425-4e0d-e2ad-9fe6ed9b5fbc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U11V9cSUhW0f",
        "outputId": "f6ce1f1e-f795-4ebd-f95d-b91a7bec66ca"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3OEo7bkhW4u"
      },
      "source": [
        "train = pd.read_csv('drive/MyDrive/CIS520 Project/train.csv')\n",
        "content = train['content'].tolist()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73xtwczJhW-y"
      },
      "source": [
        "def preprocessing(content_list):\n",
        "    \n",
        "    processed_list = []\n",
        "    \n",
        "    for line in tqdm(content_list):\n",
        "        tokens = word_tokenize(line)\n",
        "        # Convert to lower case\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        # Remove punctuation\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        # Remove remaining tokens that are not alphabetic\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        # Filter out stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        \n",
        "        processed_list.append(words)\n",
        "        \n",
        "    return processed_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYR79wyJiC4k",
        "outputId": "cc46c1d3-51bb-42b7-ff02-2217447dd90a"
      },
      "source": [
        "# Preprocessing the words\n",
        "train['processed_content'] = preprocessing(train['content'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16772/16772 [01:18<00:00, 212.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xo1RVhRiIq-"
      },
      "source": [
        "**Training Regression Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti0OOP2qiC6r"
      },
      "source": [
        "# Extract the embeddings from the stored file\n",
        "# Embedding is size 111k (# words) x 100 (dimensions)\n",
        "import os \n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('drive/MyDrive/CIS520 Project', 'word2vec_train2.txt'), encoding = 'utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKtEWZGPiC8z"
      },
      "source": [
        "# Vectorize the text samples into 2D integer tensor\n",
        "tokenizer_obj = Tokenizer()\n",
        "# Fit the tokenizer on the text\n",
        "tokenizer_obj.fit_on_texts(train['processed_content'])\n",
        "# Generate the sequence of tokens\n",
        "sequences = tokenizer_obj.texts_to_sequences(train['processed_content'])\n",
        "\n",
        "# Get the max length of each article - 5587\n",
        "max_length = max([len(s) for s in train['processed_content']])\n",
        "# Get vocab size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "# Pad the sequences\n",
        "review_pad = pad_sequences(sequences, maxlen = max_length)\n",
        "\n",
        "word_index = tokenizer_obj.word_index"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il2tcU_IiC_d",
        "outputId": "14dde701-3a67-49f6-e2b5-94e3f6f353c5"
      },
      "source": [
        "num_words = len(word_index) + 1\n",
        "words_not_found = []\n",
        "# Create the emedding matrix - map embeddings from word2vec model for each word and create matrix of word vectors\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > num_words: # Least common words (don't care)\n",
        "        continue\n",
        "        \n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    \n",
        "    if (embedding_vector is not None):\n",
        "        # Assign the ith elmenet of the embedding matrix to the embedding of that word\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "        \n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of null word embeddings: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtlyoMu0iDBZ",
        "outputId": "aae1a9a9-86d2-4ca1-9ee5-63381fdc1de8"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(111813, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaUfltKdifxF"
      },
      "source": [
        "**Training DL Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGElUT8bie6L"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Embedding, LSTM, GRU, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6BIoJTsie8l"
      },
      "source": [
        "def RNN_Regression_Model():\n",
        "    \n",
        "    text_sequence = Input(shape = (max_length,), name = 'text_sequence_input')\n",
        "    \n",
        "    rnn_layer = Embedding(num_words, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False, name = 'embedding')(text_sequence)\n",
        "    \n",
        "    # Embedding Dropout\n",
        "    rnn_layer = SpatialDropout1D(0.25, name='EMBEDDING_DROPOUT')(rnn_layer)\n",
        "    rnn_layer = GRU(units = 32, dropout = 0.2)(rnn_layer)\n",
        "    output = Dense(1, name = 'output')(rnn_layer)\n",
        "    \n",
        "    model = Model(inputs = text_sequence, outputs = output)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4KNwFnsie-s",
        "outputId": "6e18883f-d713-4b7f-94bc-f4a31a9d9ae8"
      },
      "source": [
        "model = RNN_Regression_Model()\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_sequence_input (InputLa [(None, 5587)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 5587, 100)         11181300  \n",
            "_________________________________________________________________\n",
            "EMBEDDING_DROPOUT (SpatialDr (None, 5587, 100)         0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 32)                12864     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 11,194,197\n",
            "Trainable params: 12,897\n",
            "Non-trainable params: 11,181,300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1s3iIuWifAl",
        "outputId": "718162e1-8352-4692-f404-e62173182a87"
      },
      "source": [
        "# Getting the y-variable (Quintile classification)\n",
        "\n",
        "train['quintile'] = pd.cut(train['percentile'], [0, 0.2, 0.4, 0.6, 0.8, 1], labels = [1,2,3,4,5])\n",
        "train['quintile'] = train['quintile'].astype(int)\n",
        "\n",
        "# Split into train and validation set\n",
        "VALIDATION_SPLIT = 0.2\n",
        "dl_train, dl_val = train_test_split(train, test_size = VALIDATION_SPLIT, random_state = 42, stratify = train['quintile'])\n",
        "\n",
        "train_indices = dl_train.index.tolist()\n",
        "val_indices = dl_val.index.tolist()\n",
        "\n",
        "# Get the training and validation data\n",
        "X_train = review_pad[train_indices]\n",
        "X_val = review_pad[val_indices]\n",
        "\n",
        "# NUMERICAL y-variable now\n",
        "y_train = dl_train['percentile'].to_numpy()\n",
        "y_val = dl_val['percentile'].to_numpy()\n",
        "\n",
        "\n",
        "print('Shape of X_train: ', X_train.shape)\n",
        "print('Shape of y_train: ', y_train.shape)\n",
        "print('Shape of X_val: ', X_val.shape)\n",
        "print('Shape of y_val: ', y_val.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (13417, 5587)\n",
            "Shape of y_train:  (13417,)\n",
            "Shape of X_val:  (3355, 5587)\n",
            "Shape of y_val:  (3355,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuUPAE3RiDFg",
        "outputId": "18bfa79f-0352-446c-bb74-d8508c2e4953"
      },
      "source": [
        "# Early stopping and model checkpoint\n",
        "early_stopping = EarlyStopping(monitor = 'val_mean_absolute_error', patience = 10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'drive/MyDrive/CIS520 Project/word2vec_gru_content_reg.h5', monitor='val_mean_absolute_error', verbose=0, save_best_only=True)\n",
        "\n",
        "# Train the DL Model\n",
        "model.compile(loss = 'mean_absolute_error', optimizer = 'adam', metrics = ['mean_absolute_error'])\n",
        "\n",
        "model.fit(X_train, y_train, batch_size = 32, epochs = 50, validation_data = (X_val, y_val), verbose = 1,\n",
        "         callbacks = [early_stopping, model_checkpoint])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "420/420 [==============================] - 84s 201ms/step - loss: 0.1873 - mean_absolute_error: 0.1873 - val_loss: 0.1792 - val_mean_absolute_error: 0.1792\n",
            "Epoch 2/50\n",
            "420/420 [==============================] - 82s 194ms/step - loss: 0.1847 - mean_absolute_error: 0.1847 - val_loss: 0.1874 - val_mean_absolute_error: 0.1874\n",
            "Epoch 3/50\n",
            "420/420 [==============================] - 84s 201ms/step - loss: 0.1833 - mean_absolute_error: 0.1833 - val_loss: 0.1774 - val_mean_absolute_error: 0.1774\n",
            "Epoch 4/50\n",
            "420/420 [==============================] - 83s 197ms/step - loss: 0.1822 - mean_absolute_error: 0.1822 - val_loss: 0.1788 - val_mean_absolute_error: 0.1788\n",
            "Epoch 5/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1813 - mean_absolute_error: 0.1813 - val_loss: 0.1835 - val_mean_absolute_error: 0.1835\n",
            "Epoch 6/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1800 - mean_absolute_error: 0.1800 - val_loss: 0.1815 - val_mean_absolute_error: 0.1815\n",
            "Epoch 7/50\n",
            "420/420 [==============================] - 87s 208ms/step - loss: 0.1795 - mean_absolute_error: 0.1795 - val_loss: 0.1769 - val_mean_absolute_error: 0.1769\n",
            "Epoch 8/50\n",
            "420/420 [==============================] - 88s 209ms/step - loss: 0.1781 - mean_absolute_error: 0.1781 - val_loss: 0.1747 - val_mean_absolute_error: 0.1747\n",
            "Epoch 9/50\n",
            "420/420 [==============================] - 87s 208ms/step - loss: 0.1776 - mean_absolute_error: 0.1776 - val_loss: 0.1765 - val_mean_absolute_error: 0.1765\n",
            "Epoch 10/50\n",
            "420/420 [==============================] - 87s 207ms/step - loss: 0.1771 - mean_absolute_error: 0.1771 - val_loss: 0.1805 - val_mean_absolute_error: 0.1805\n",
            "Epoch 11/50\n",
            "420/420 [==============================] - 86s 205ms/step - loss: 0.1747 - mean_absolute_error: 0.1747 - val_loss: 0.1789 - val_mean_absolute_error: 0.1789\n",
            "Epoch 12/50\n",
            "420/420 [==============================] - 89s 212ms/step - loss: 0.1744 - mean_absolute_error: 0.1744 - val_loss: 0.1739 - val_mean_absolute_error: 0.1739\n",
            "Epoch 13/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1744 - mean_absolute_error: 0.1744 - val_loss: 0.1766 - val_mean_absolute_error: 0.1766\n",
            "Epoch 14/50\n",
            "420/420 [==============================] - 85s 202ms/step - loss: 0.1740 - mean_absolute_error: 0.1740 - val_loss: 0.1745 - val_mean_absolute_error: 0.1745\n",
            "Epoch 15/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1731 - mean_absolute_error: 0.1731 - val_loss: 0.1782 - val_mean_absolute_error: 0.1782\n",
            "Epoch 16/50\n",
            "420/420 [==============================] - 85s 202ms/step - loss: 0.1714 - mean_absolute_error: 0.1714 - val_loss: 0.1761 - val_mean_absolute_error: 0.1761\n",
            "Epoch 17/50\n",
            "420/420 [==============================] - 84s 200ms/step - loss: 0.1722 - mean_absolute_error: 0.1722 - val_loss: 0.1772 - val_mean_absolute_error: 0.1772\n",
            "Epoch 18/50\n",
            "420/420 [==============================] - 84s 201ms/step - loss: 0.1710 - mean_absolute_error: 0.1710 - val_loss: 0.1754 - val_mean_absolute_error: 0.1754\n",
            "Epoch 19/50\n",
            "420/420 [==============================] - 87s 207ms/step - loss: 0.1713 - mean_absolute_error: 0.1713 - val_loss: 0.1725 - val_mean_absolute_error: 0.1725\n",
            "Epoch 20/50\n",
            "420/420 [==============================] - 85s 202ms/step - loss: 0.1713 - mean_absolute_error: 0.1713 - val_loss: 0.1817 - val_mean_absolute_error: 0.1817\n",
            "Epoch 21/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1717 - mean_absolute_error: 0.1717 - val_loss: 0.1783 - val_mean_absolute_error: 0.1783\n",
            "Epoch 22/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1692 - mean_absolute_error: 0.1692 - val_loss: 0.1768 - val_mean_absolute_error: 0.1768\n",
            "Epoch 23/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1692 - mean_absolute_error: 0.1692 - val_loss: 0.1729 - val_mean_absolute_error: 0.1729\n",
            "Epoch 24/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1693 - mean_absolute_error: 0.1693 - val_loss: 0.1744 - val_mean_absolute_error: 0.1744\n",
            "Epoch 25/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1779 - mean_absolute_error: 0.1779 - val_loss: 0.1790 - val_mean_absolute_error: 0.1790\n",
            "Epoch 26/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1678 - mean_absolute_error: 0.1678 - val_loss: 0.1821 - val_mean_absolute_error: 0.1821\n",
            "Epoch 27/50\n",
            "420/420 [==============================] - 85s 202ms/step - loss: 0.1680 - mean_absolute_error: 0.1680 - val_loss: 0.1792 - val_mean_absolute_error: 0.1792\n",
            "Epoch 28/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1677 - mean_absolute_error: 0.1677 - val_loss: 0.1763 - val_mean_absolute_error: 0.1763\n",
            "Epoch 29/50\n",
            "420/420 [==============================] - 85s 203ms/step - loss: 0.1666 - mean_absolute_error: 0.1666 - val_loss: 0.1784 - val_mean_absolute_error: 0.1784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6c482a3c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2djxslswCpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c7a3a8-7a69-4c3c-bddc-f72d702f8b63"
      },
      "source": [
        "# Save model\n",
        "model.save('drive/MyDrive/CIS520 Project/word2vec_gru_content_reg1')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: drive/MyDrive/CIS520 Project/word2vec_gru_content_reg1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qivgPNup7DJh"
      },
      "source": [
        "# Load model\n",
        "model = keras.models.load_model('drive/MyDrive/CIS520 Project/word2vec_gru_content1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ-oyBZZsT9Y"
      },
      "source": [
        "**Evaluating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BNWEI4GsS2k"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhPD0gdTsS7j"
      },
      "source": [
        "# Predict on the validation data - returns (3355, 5) matrix of predicted classes\n",
        "val_probs = model.predict(X_val)\n",
        "# Predicted quintiles\n",
        "val_preds = np.argmax(val_probs, axis = 1)\n",
        "\n",
        "y_val_actual = np.argmax(y_val, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypw2oWssv6Ws"
      },
      "source": [
        "**Model Interpretability using ELI5**\n",
        "\n",
        "(Needs to be installed first)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRH9hi1dyhUf"
      },
      "source": [
        "import eli5\n",
        "from eli5.lime import TextExplainer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APNHBXZ6tEyy"
      },
      "source": [
        "# Define the custom predict function - input is list of strings (documents) and return matrix of shape (n_samples, n_classes) with probability values\n",
        "\n",
        "\n",
        "# Assumes you already fitted the tokenizer on the training data\n",
        "def predict_complex(documents_list):\n",
        "\n",
        "  # Generate the sequence of tokens\n",
        "  sequences = tokenizer_obj.texts_to_sequences(documents_list)\n",
        "\n",
        "  # Pad the sequences\n",
        "  X = pad_sequences(sequences, maxlen = 5587)\n",
        "\n",
        "  # Predict\n",
        "  y_preds = model.predict([X], batch_size = 32, verbose = 0)\n",
        "\n",
        "  # *** Convert this into a one-class classification of bottom 3 quintiles vs top 2 quintiles\n",
        "  y_high = y_probs[:, 3:].sum(axis = 1)\n",
        "  y_low = y_probs[:, 0:3].sum(axis = 1)\n",
        "\n",
        "  y_out = np.vstack((y_low, y_high)).T\n",
        "  return y_out\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "DYojM6wPtE1D",
        "outputId": "c37af93d-212d-420a-a53b-1e06aff696d1"
      },
      "source": [
        "te = TextExplainer(random_state = 42)\n",
        "\n",
        "doc = ' '.join(dl_val['processed_content'].iloc[1])\n",
        "te.fit(doc, predict_complex)\n",
        "te.explain_prediction()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6733decc421b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/eli5/lime/lime.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, doc, predict_proba)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0my_proba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_proba_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mexpand_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         )\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/eli5/lime/lime.py\u001b[0m in \u001b[0;36m_train_local_classifier\u001b[0;34m(estimator, samples, similarity, y_proba, expand_factor, test_size, random_state)\u001b[0m\n\u001b[1;32m    365\u001b[0m               \u001b[0mexpand_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimilarity_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m               random_state=rng)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0my_proba_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/eli5/lime/utils.py\u001b[0m in \u001b[0;36mfit_proba\u001b[0;34m(clf, X, y_proba, expand_factor, sample_weight, shuffle, random_state, **fit_params)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwith_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/eli5/lime/utils.py\u001b[0m in \u001b[0;36mexpanded_X_y_sample_weights\u001b[0;34m(X, y_proba, expand_factor, sample_weight, shuffle, random_state)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                                       \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                                                       extra_arrays=[\n\u001b[0;32m--> 111\u001b[0;31m                                                           \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                                                       ]))\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/eli5/lime/utils.py\u001b[0m in \u001b[0;36mexpand_dataset\u001b[0;34m(X, y_proba, factor, random_state, extra_arrays)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "z-_nPc0_8LND",
        "outputId": "2665c000-2c2b-4ad8-8ca5-05d5afbbdace"
      },
      "source": [
        "te.explain_weights(target_names = ['low', 'high'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "            \n",
              "                \n",
              "                \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=high\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.34%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.465\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        back\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.92%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.228\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        penn\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.24%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.213\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        arts sciences\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.45%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.204\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        tweet\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.57%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.199\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        said\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.67%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.194\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        received\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.78%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.189\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        emergency\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.99%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.180\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        university\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.16%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.173\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        pm\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.21%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.171\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        operations\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.51%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.158\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        according\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.64%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.153\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        noon\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.64%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 62 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.70%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 58 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.70%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.151\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        director student\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.60%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.155\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        close\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.95%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.182\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        director\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.90%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.229\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        events\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.87%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.230\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        around\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.20%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.262\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        philadelphia\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 91.38%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.302\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        emergency information\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.005\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "            \n",
              "        \n",
              "\n",
              "        \n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "Explanation(estimator=\"SGDClassifier(alpha=0.001, average=False, class_weight=None,\\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\\n              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\\n              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\\n              power_t=0.5, random_state=RandomState(MT19937) at 0x7F1BB8E1AA98,\\n              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\\n              warm_start=False)\", description=\"\\nFeatures with largest coefficients.\\nCaveats:\\n1. Be careful with features which are not\\n   independent - weights don't show their importance.\\n2. If scale of input features is different then scale of coefficients\\n   will also be different, making direct comparison between coefficient values\\n   incorrect.\\n3. Depending on regularization, rare features sometimes may have high\\n   coefficients; this doesn't mean they contribute much to the\\n   classification result for most examples.\\n\", error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target='high', feature_weights=FeatureWeights(pos=[FeatureWeight(feature='back', weight=0.46460585912514923, std=None, value=None), FeatureWeight(feature='penn', weight=0.22812865635742785, std=None, value=None), FeatureWeight(feature='arts sciences', weight=0.21348123617734024, std=None, value=None), FeatureWeight(feature='tweet', weight=0.20402853180754027, std=None, value=None), FeatureWeight(feature='said', weight=0.19874737681116383, std=None, value=None), FeatureWeight(feature='received', weight=0.1943734321213178, std=None, value=None), FeatureWeight(feature='emergency', weight=0.1893864051431553, std=None, value=None), FeatureWeight(feature='university', weight=0.18016843451751283, std=None, value=None), FeatureWeight(feature='pm', weight=0.17296360130298763, std=None, value=None), FeatureWeight(feature='operations', weight=0.17107896490725139, std=None, value=None), FeatureWeight(feature='according', weight=0.15829456811253237, std=None, value=None), FeatureWeight(feature='noon', weight=0.15305975321772025, std=None, value=None)], neg=[FeatureWeight(feature='<BIAS>', weight=-1.0048567422168526, std=None, value=None), FeatureWeight(feature='emergency information', weight=-0.30216246084592135, std=None, value=None), FeatureWeight(feature='philadelphia', weight=-0.26184173360030305, std=None, value=None), FeatureWeight(feature='around', weight=-0.23044809040924957, std=None, value=None), FeatureWeight(feature='events', weight=-0.22890388819604798, std=None, value=None), FeatureWeight(feature='director', weight=-0.1818762428200143, std=None, value=None), FeatureWeight(feature='close', weight=-0.15490537352427183, std=None, value=None), FeatureWeight(feature='director student', weight=-0.15052744157363443, std=None, value=None)], pos_remaining=62, neg_remaining=58), proba=None, score=None, weighted_spans=None, heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}